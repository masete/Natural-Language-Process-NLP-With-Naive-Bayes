{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2eae9e4-e9ef-419d-98c7-899aa8b83aca",
   "metadata": {},
   "source": [
    "# Therapy Chatbot\n",
    "\n",
    "The dataset contains 80 user responses, in the response_text column, to a therapy chatbot. Bot said: 'Describe a time when you have acted as a resource for someone else'. User responded. If a response is 'not flagged', the user can continue talking to the bot. If it is 'flagged', the user is referred to help. We are going to predict if it is flagged or not according to users responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92615176-e844-49e4-a2f6-d72c2ebfc3ed",
   "metadata": {},
   "source": [
    "### Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b09ba12e-a77a-403f-b3f8-57ebac0db484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nicholas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7371fa-2817-4003-bd0a-3dccebced61c",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1415a85-5d39-4d06-8b45-3dc1500df207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flagged</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flagged</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>a couple of years ago my friends was going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flagged</td>\n",
       "      <td>Roommate when he was going through death and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flagged</td>\n",
       "      <td>i've had a couple of friends (you could say mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Listened to someone talk about relationship tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>flagged</td>\n",
       "      <td>I will always listen. I comforted my sister wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                      response_text\n",
       "0  not_flagged              I try and avoid this sort of conflict\n",
       "1      flagged  Had a friend open up to me about his mental ad...\n",
       "2      flagged  I saved a girl from suicide once. She was goin...\n",
       "3  not_flagged  i cant think of one really...i think i may hav...\n",
       "4  not_flagged  Only really one friend who doesn't fit into th...\n",
       "5  not_flagged  a couple of years ago my friends was going to ...\n",
       "6      flagged  Roommate when he was going through death and l...\n",
       "7      flagged  i've had a couple of friends (you could say mo...\n",
       "8  not_flagged  Listened to someone talk about relationship tr...\n",
       "9      flagged  I will always listen. I comforted my sister wh..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/Sheet_1.csv\",encoding= \"latin1\" )\n",
    "# data.drop([\"Unnamed: 3\",\"Unnamed: 4\",\"Unnamed: 5\",\n",
    "#            \"Unnamed: 6\",\"Unnamed: 7\",], axis = 1, inplace =True)\n",
    "data = pd.concat([data[\"class\"],data[\"response_text\"]], axis = 1)\n",
    "\n",
    "data.dropna(axis=0, inplace =True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f90d8-54c2-4f57-b42d-988260ce5a3c",
   "metadata": {},
   "source": [
    "### 0 to Not Flagged and 1 to Flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54734af6-25fa-4943-bcb5-0d771881e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                      response_text\n",
       "0      0              I try and avoid this sort of conflict\n",
       "1      1  Had a friend open up to me about his mental ad...\n",
       "2      1  I saved a girl from suicide once. She was goin...\n",
       "3      0  i cant think of one really...i think i may hav...\n",
       "4      0  Only really one friend who doesn't fit into th..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"class\"] = [1 if each == \"flagged\" else 0 for each in data[\"class\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01fbbad8-c129-4499-8418-2c58a28fbb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have helped advise friends who have faced circumstances similar to mine'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.response_text[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69475603-4820-4f93-aba9-800edad7e39f",
   "metadata": {},
   "source": [
    "### Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd6096-bcfb-4c5c-933e-b14c50302fcb",
   "metadata": {},
   "source": [
    "We can remove non-letter characters in our text with Regular Expression method.\n",
    "The lower() methods returns the lowercased string from the given string. It converts all uppercase characters to lowercase. If no uppercase characters exist, it returns the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19d94a41-6dd9-4015-b0bf-2ce97271adee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have helped advise friends who have faced circumstances similar to mine\n"
     ]
    }
   ],
   "source": [
    "first_text = data.response_text[16]\n",
    "text = re.sub(\"[^a-zA-Z]\",\" \",first_text)\n",
    "text = text.lower() \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913c8b5-8a53-4e1f-87dd-bfeba902d2c3",
   "metadata": {},
   "source": [
    "### Irrelevant Words (Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cc43648-6936-4781-9f27-a601bbae7e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['helped', 'advise', 'friends', 'faced', 'circumstances', 'similar', 'mine']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(text)\n",
    "text = [ word for word in text if not word in set(stopwords.words(\"english\"))]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8c0ac-0949-45db-b943-68bd44a5c1b1",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e2193c8-f09b-4b8b-808c-c3527e383bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'advise', 'friend', 'face', 'circumstance', 'similar', 'mine']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = [(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, \"n\"),pos = \"v\"),pos=\"a\")) for word in text]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00421564-e29c-4d81-a09a-e0e465b7f15a",
   "metadata": {},
   "source": [
    "### All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ec05490-d441-4d5a-8789-d3c33ee9f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = []\n",
    "for description in data.response_text:\n",
    "       \n",
    "    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
    "    description = description.lower() \n",
    "    \n",
    "    description = nltk.word_tokenize(description)\n",
    "    description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    description = (lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, \"n\"),pos = \"v\"),pos=\"a\") for word in description)\n",
    "    \n",
    "    description = \" \".join(description)\n",
    "    description_list.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84e542ef-7b77-4a85-94e5-187b14199866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'help advise friend face circumstance similar mine'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_list[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef97432-6fde-4bb4-808a-3d59b8774d15",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc4eaa48-28ca-4194-9a12-f40b5c62820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently used 500 words ['able' 'absolutely' 'acquaintance' 'act' 'action' 'activity' 'addiction'\n",
      " 'adequate' 'admit' 'advice' 'advise' 'age' 'ago' 'agony' 'alcoholic'\n",
      " 'allow' 'anniversary' 'answer' 'anxiety' 'anxious' 'appose' 'ask'\n",
      " 'attention' 'aunt' 'avoid' 'away' 'bad' 'basically' 'bedroom' 'best'\n",
      " 'big' 'bite' 'blow' 'blue' 'blunt' 'book' 'boyfriend' 'break' 'bring'\n",
      " 'brother' 'bunch' 'calm' 'camp' 'campsite' 'cancer' 'car' 'care' 'catch'\n",
      " 'category' 'cause' 'chance' 'change' 'chat' 'circumstance' 'clean'\n",
      " 'cocaine' 'come' 'comfort' 'commit' 'common' 'complete' 'completely'\n",
      " 'concern' 'confine' 'conflict' 'convince' 'cop' 'cope' 'counselor'\n",
      " 'countless' 'couple' 'crazy' 'cut' 'cutter' 'damn' 'date' 'day' 'deal'\n",
      " 'death' 'define' 'depress' 'depression' 'desire' 'diagnose' 'dialog'\n",
      " 'die' 'difficulty' 'disorder' 'doc' 'dont' 'douche' 'drag' 'drive' 'drug'\n",
      " 'dump' 'ear' 'early' 'email' 'emotional' 'encourage' 'end' 'entire'\n",
      " 'essential' 'esteem' 'eventually' 'everyday' 'ex' 'example' 'excite'\n",
      " 'experience' 'express' 'extremely' 'face' 'facebook' 'fact' 'fairly'\n",
      " 'family' 'father' 'feel' 'fell' 'felt' 'fight' 'fit' 'fix' 'flicker'\n",
      " 'focus' 'friend' 'frustrate' 'ged' 'gf' 'girl' 'girlfriend' 'goal' 'good'\n",
      " 'grade' 'grandmother' 'guess' 'guy' 'hadnt' 'haha' 'half' 'hand' 'hang'\n",
      " 'happen' 'hard' 'harm' 'head' 'heal' 'health' 'hear' 'help' 'helpful'\n",
      " 'hesitate' 'high' 'hit' 'hold' 'home' 'homeless' 'honest' 'hood' 'hop'\n",
      " 'hope' 'horrable' 'hospital' 'hour' 'house' 'huge' 'human' 'hurt' 'idk'\n",
      " 'important' 'indirectly' 'ing' 'initiate' 'innermost' 'input' 'inside'\n",
      " 'intense' 'internet' 'irl' 'isolate' 'issue' 'jokingly' 'judge' 'junior'\n",
      " 'kid' 'kill' 'kind' 'kindness' 'know' 'knowledge' 'lack' 'late' 'laugh'\n",
      " 'le' 'lend' 'let' 'level' 'life' 'light' 'like' 'line' 'listen'\n",
      " 'listener' 'little' 'live' 'logical' 'long' 'look' 'lose' 'loss' 'lot'\n",
      " 'love' 'low' 'major' 'make' 'manage' 'maybe' 'meet' 'memorial' 'men'\n",
      " 'mental' 'method' 'mind' 'mom' 'month' 'mother' 'mt' 'nah' 'naturally'\n",
      " 'necessarily' 'need' 'nervous' 'nice' 'night' 'normal' 'number'\n",
      " 'objective' 'obtain' 'occurrence' 'od' 'offer' 'oh' 'ok' 'open'\n",
      " 'openness' 'overcome' 'pack' 'parent' 'past' 'path' 'peace' 'people'\n",
      " 'perfect' 'period' 'person' 'personal' 'physical' 'pick' 'pill'\n",
      " 'positive' 'possible' 'possibly' 'pretty' 'probably' 'problem' 'progress'\n",
      " 'promise' 'provide' 'psych' 'pull' 'purpose' 'qualify' 'question' 'quite'\n",
      " 'rant' 'rational' 'read' 'reality' 'realize' 'really' 'recovery' 'refer'\n",
      " 'reflect' 'rehab' 'reject' 'relate' 'relationship' 'relief' 'remember'\n",
      " 'remind' 'remote' 'resource' 'respect' 'restless' 'result' 'roommate'\n",
      " 'rude' 'sad' 'save' 'saw' 'say' 'school' 'schoolwork' 'self' 'sense'\n",
      " 'set' 'severe' 'share' 'shelter' 'shit' 'shortly' 'sign' 'similar'\n",
      " 'simply' 'sister' 'situation' 'skip' 'slightly' 'slowly' 'somebody'\n",
      " 'sort' 'speak' 'specific' 'specifically' 'spend' 'spiral' 'spot'\n",
      " 'stability' 'start' 'stay' 'stop' 'story' 'strange' 'struggle' 'stuff'\n",
      " 'subject' 'suffer' 'suicidal' 'suicide' 'summer' 'super' 'support'\n",
      " 'supportive' 'sure' 'surf' 'survival' 'sustain' 'swallow' 'sway' 'swim'\n",
      " 'switch' 'talk' 'teacher' 'tell' 'thankgiving' 'thats' 'therapist'\n",
      " 'therapy' 'theripist' 'thing' 'think' 'throw' 'till' 'time' 'tough'\n",
      " 'trap' 'treat' 'treatment' 'trouble' 'truth' 'try' 'tryin' 'tunnel'\n",
      " 'turmoil' 'tutor' 'twice' 'type' 'understand' 'unfortunately' 'unless'\n",
      " 'use' 'vent' 'verge' 'virgity' 'visit' 'walk' 'want' 'ward' 'way' 'weed'\n",
      " 'week' 'wood' 'work' 'write' 'year' 'yearbook']\n"
     ]
    }
   ],
   "source": [
    "# max_features = 100\n",
    "# count_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\n",
    "# print(\"Top {} Most Used Words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "max_feature = 500\n",
    "\n",
    "cv = CountVectorizer(max_features = max_feature, stop_words = \"english\")\n",
    "\n",
    "sparce_matrix = cv.fit_transform(description_list).toarray() # x\n",
    "\n",
    "print(\"Most frequently used {} words {}\".format(max_feature, cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d6af0-6b28-4a92-ab74-65e9ecc19ae7",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14290128-020a-4ffa-b742-e28848e94ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,0].values\n",
    "x = sparce_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0ec1b-ef67-4c92-9c46-ba93dd112c75",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34b958eb-4b43-45f7-9ecb-b5f95e5281b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ae86f-d864-484c-a114-4a9cc594fc2f",
   "metadata": {},
   "source": [
    "#### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6641f886-0af6-46a9-ac3c-33b6dac74935",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but GaussianNB is expecting 397 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m nb\u001b[38;5;241m.\u001b[39mfit(x_train,y_train)\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(nb\u001b[38;5;241m.\u001b[39mscore(y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m),y_test),\u001b[38;5;241m2\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:80\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:249\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1 features, but GaussianNB is expecting 397 features as input."
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "y_pred = nb.predict(x_test)\n",
    "print(\"Accuracy: {}\".format(round(nb.score(y_pred.reshape(-1,1),y_test),2)))\n",
    "\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import metrics\n",
    "# gnb = GaussianNB()\n",
    "\n",
    "# gnb.fit(x_train,y_train)\n",
    "\n",
    "# y_pred = gnb.predict(x_test)\n",
    "\n",
    "# print('The accuracy of the Random Forest is',metrics.accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4eba4c-ad23-4bab-b57c-570bf024205c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c162e47-54d7-444e-aec7-77acee8f4074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeda115-6cc4-4ebe-8c5c-998789ff87d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46b408-3109-4ac5-9f10-7511fea6e8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
